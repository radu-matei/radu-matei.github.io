<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ml on radu's blog</title><link>https://radu-matei.com/tags/ml/</link><description>Recent content in Ml on radu's blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><managingEditor>radu@matei.ai (Radu Matei)</managingEditor><webMaster>radu@matei.ai (Radu Matei)</webMaster><copyright>Radu Matei</copyright><lastBuildDate>Tue, 05 Sep 2023 16:00:00 +0000</lastBuildDate><atom:link href="https://radu-matei.com/tags/ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Introducing Fermyon Serverless AI</title><link>https://radu-matei.com/blog/introducing-fermyon-serverless-ai/</link><pubDate>Tue, 05 Sep 2023 16:00:00 +0000</pubDate><author>radu@matei.ai (Radu Matei)</author><guid>https://radu-matei.com/blog/introducing-fermyon-serverless-ai/</guid><description>Fermyon Serverless AI gives you the building blocks for integrating Artificial Intelligence into your serverless applications, with AI inferencing for Large Language Models (LLMs) for Llama2 and CodeLlama, support for generating sentence embeddings and storing them in a vector-ready database, built-in key-value storage, and a seamless developer experience.</description></item><item><title>Neural network inferencing for PyTorch and TensorFlow with ONNX, WebAssembly System Interface, and WASI NN</title><link>https://radu-matei.com/blog/wasi-nn-onnx/</link><pubDate>Wed, 07 Jul 2021 00:00:00 +0000</pubDate><author>radu@matei.ai (Radu Matei)</author><guid>https://radu-matei.com/blog/wasi-nn-onnx/</guid><description>WASI NN is a proposal that allows WebAssembly guest modules running outside the browser to perform neural network inferencing by using host-provided implementations that can leverage CPU multi-threading, host optimizations, or hardware devices such as GPUs or TPUs. This article explores the goals of WASI NN, existing implementations, and details a new experimental implementation targeting ONNX, the Open Neural Network Exchange , which allows the either usage of models built with PyTorch or TensorFlow from guest WebAssembly modules.</description></item><item><title>TensorFlow inferencing using WebAssembly and WASI</title><link>https://radu-matei.com/blog/tensorflow-inferencing-wasi/</link><pubDate>Sun, 18 Oct 2020 00:00:00 +0000</pubDate><author>radu@matei.ai (Radu Matei)</author><guid>https://radu-matei.com/blog/tensorflow-inferencing-wasi/</guid><description>In this article, we experiment with building a Rust program that performs image classification using the MobileNet V2 TensorFlow model, compile it to WebAssembly, and instantiate the module using two WebAssembly runtimes that use the WebAssembly System Interface (WASI), the native NodeJS WASI runtime, and Wasmtime. A special interest is given to writing model and image data into the moduleâ€™s linear memory, with implementations in both JavaScript and Rust. Finally, a simple prediction API is exemplified running on top of the Wasmtime runtime, and some limitations of this approach are discussed.</description></item></channel></rss>
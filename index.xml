<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>radu&#39;s blog</title>
    <link>https://radu-matei.com/</link>
    <description>Recent content on radu&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; &lt;a href=&#34;https://twitter.com/matei_radu&#34; rel=&#34;noreferrer&#34; target=&#34;_blank&#34;&gt;Radu Matei&lt;/a&gt; 2023</copyright>
    <lastBuildDate>Wed, 22 Mar 2023 16:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://radu-matei.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    
    
    <item>
      <title>Spin 1.0 â€” The Developer Tool for Serverless WebAssembly</title>
      <link>https://radu-matei.com/blog/introducing-spin-v1/</link>
      <pubDate>Wed, 22 Mar 2023 16:00:00 +0000</pubDate>
      
      <guid>https://radu-matei.com/blog/introducing-spin-v1/</guid>
      <description>Introducing the first stable release for Spin, the open source developer tool for building, distributing, and running serverless applications built with WebAssembly.</description>
    </item>
    
    <item>
      <title>The Six Ways of Optimizing WebAssembly</title>
      <link>https://radu-matei.com/blog/six-ways-optimize-webassembly/</link>
      <pubDate>Thu, 26 Jan 2023 02:00:00 +0000</pubDate>
      
      <guid>https://radu-matei.com/blog/six-ways-optimize-webassembly/</guid>
      <description>This post is co-authored with Matt Butcher, and it originally appeared on InfoQ.com .
Key Takeaways While many languages support Wasm, some are faster than others. Some compilers natively support optimizing Wasm for efficiency and speed. The wasm-opt tool can optimize a Wasm binary regardless of the original language it was used to create it. Using a JIT-enabled runtime can improve runtime performance depending on the hardware platform you are using.</description>
    </item>
    
    <item>
      <title>Building host implementations for WebAssembly interfaces</title>
      <link>https://radu-matei.com/blog/wasm-components-host-implementations/</link>
      <pubDate>Thu, 20 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://radu-matei.com/blog/wasm-components-host-implementations/</guid>
      <description>WebAssembly and WASI show great promise for the future of computing outside the browser, and the WebAssembly component model aims to improve the portability, cross-language, and composition story for Wasm. This article introduces host implementations, and how to use tooling from the Bytecode Alliance to build and use such implementations.</description>
    </item>
    
    <item>
      <title>Introduction to WebAssembly components</title>
      <link>https://radu-matei.com/blog/intro-wasm-components/</link>
      <pubDate>Thu, 16 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://radu-matei.com/blog/intro-wasm-components/</guid>
      <description>WebAssembly and WASI show great promise for the future of computing outside the browser, and the WebAssembly component model aims to improve the portability, cross-language, and composition story for Wasm. This article explains the goals of the component model, and showcases how to use tooling from the Bytecode Alliance to build and consume such components.</description>
    </item>
    
    <item>
      <title>Neural network inferencing  for PyTorch and TensorFlow with ONNX, WebAssembly System Interface, and WASI NN</title>
      <link>https://radu-matei.com/blog/wasi-nn-onnx/</link>
      <pubDate>Wed, 07 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://radu-matei.com/blog/wasi-nn-onnx/</guid>
      <description>WASI NN is a proposal that allows WebAssembly guest modules running outside the browser to perform neural network inferencing by using host-provided implementations that can leverage CPU multi-threading, host optimizations, or hardware devices such as GPUs or TPUs. This article explores the goals of WASI NN, existing implementations, and details a new experimental implementation targeting ONNX, the Open Neural Network Exchange , which allows the either usage of models built with PyTorch or TensorFlow from guest WebAssembly modules.</description>
    </item>
    
  </channel>
</rss>

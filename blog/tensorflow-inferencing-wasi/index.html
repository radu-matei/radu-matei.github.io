<!doctype html><html class="not-ready text-sm lg:text-base" style=--bg:#faf6f1 lang=en-us><head><link rel=preload as=font href=/fonts/spin-quasi-regular-subset.woff2 type=font/woff2 crossorigin=anonymous><link rel=preload as=font href=/fonts/spin-mono-regular-subset.woff2 type=font/woff2 crossorigin=anonymous><style>@font-face{font-family:spin quasi;src:url(/fonts/spin-quasi-regular-subset.woff2)format('woff2');font-display:swap}@font-face{font-family:spin mono;src:url(/fonts/spin-mono-regular-subset.woff2)format('woff2');font-display:swap}html{font-family:spin quasi,ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,segoe ui,Roboto,helvetica neue,Arial,noto sans,sans-serif,apple color emoji,segoe ui emoji,segoe ui symbol,noto color emoji}code,kbd,samp,pre{font-family:spin mono,ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,liberation mono,courier new,monospace}</style><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>TensorFlow inferencing using WebAssembly and WASI - radu's blog</title>
<meta name=theme-color><link rel=canonical href=https://radu-matei.com/blog/tensorflow-inferencing-wasi/><meta name=description content="In this article, we experiment with building a Rust program that performs image classification using the MobileNet V2 TensorFlow model, compile it to WebAssembly, and instantiate the module using two WebAssembly runtimes that use the WebAssembly System Interface (WASI), the native NodeJS WASI runtime, and Wasmtime. A special interest is given to writing model and image data into the module’s linear memory, with implementations in both JavaScript and Rust. Finally, a simple prediction API is exemplified running on top of the Wasmtime runtime, and some limitations of this approach are discussed."><meta name=author content="Radu Matei"><link rel="preload stylesheet" as=style href=https://radu-matei.com/main.min.css><script defer src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.3/highlight.min.js onload=hljs.highlightAll()></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/base16/atelier-estuary-light.min.css media=screen><link disabled id=hljs-dark rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/base16/gruvbox-dark-hard.min.css media=screen><link rel=preload as=image href=https://radu-matei.com/theme.svg><link rel=preload as=image href=https://radu-matei.com/twitter.svg><link rel=preload as=image href=https://radu-matei.com/github.svg><link rel=icon href=https://radu-matei.com/favicon.ico><link rel=apple-touch-icon href=https://radu-matei.com/apple-touch-icon.png><meta name=generator content="Hugo 0.124.1"><meta property="og:title" content="TensorFlow inferencing using WebAssembly and WASI"><meta property="og:description" content="In this article, we experiment with building a Rust program that performs image classification using the MobileNet V2 TensorFlow model, compile it to WebAssembly, and instantiate the module using two WebAssembly runtimes that use the WebAssembly System Interface (WASI), the native NodeJS WASI runtime, and Wasmtime. A special interest is given to writing model and image data into the module’s linear memory, with implementations in both JavaScript and Rust. Finally, a simple prediction API is exemplified running on top of the Wasmtime runtime, and some limitations of this approach are discussed."><meta property="og:type" content="article"><meta property="og:url" content="https://radu-matei.com/blog/tensorflow-inferencing-wasi/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2020-10-18T00:00:00+00:00"><meta property="article:modified_time" content="2020-10-18T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="TensorFlow inferencing using WebAssembly and WASI"><meta name=twitter:description content="In this article, we experiment with building a Rust program that performs image classification using the MobileNet V2 TensorFlow model, compile it to WebAssembly, and instantiate the module using two WebAssembly runtimes that use the WebAssembly System Interface (WASI), the native NodeJS WASI runtime, and Wasmtime. A special interest is given to writing model and image data into the module’s linear memory, with implementations in both JavaScript and Rust. Finally, a simple prediction API is exemplified running on top of the Wasmtime runtime, and some limitations of this approach are discussed."><meta itemprop=name content="TensorFlow inferencing using WebAssembly and WASI"><meta itemprop=description content="In this article, we experiment with building a Rust program that performs image classification using the MobileNet V2 TensorFlow model, compile it to WebAssembly, and instantiate the module using two WebAssembly runtimes that use the WebAssembly System Interface (WASI), the native NodeJS WASI runtime, and Wasmtime. A special interest is given to writing model and image data into the module’s linear memory, with implementations in both JavaScript and Rust. Finally, a simple prediction API is exemplified running on top of the Wasmtime runtime, and some limitations of this approach are discussed."><meta itemprop=datePublished content="2020-10-18T00:00:00+00:00"><meta itemprop=dateModified content="2020-10-18T00:00:00+00:00"><meta itemprop=wordCount content="2881"><meta itemprop=keywords content="wasm,rust,tensorflow,ml,"></head><body class="text-black duration-200 ease-out dark:text-white"><header class="mx-auto flex h-[5rem] max-w-3xl px-8 lg:justify-center"><div class="relative z-50 mr-auto flex items-center"><a class="-translate-x-[1px] -translate-y-[1px] text-2xl font-semibold" href=https://radu-matei.com/>radu's blog</a><div class="btn-dark text-[0] ml-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.svg)_left_center/cover_no-repeat] dark:invert dark:[background-position:right]" role=button aria-label=Dark></div></div><div class="btn-menu relative z-50 -mr-8 flex h-[5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden" role=button aria-label=Menu></div><script>const htmlClass=document.documentElement.classList;setTimeout(()=>{htmlClass.remove("not-ready")},10);const hljsDark=document.getElementById("hljs-dark"),btnMenu=document.querySelector(".btn-menu");btnMenu.addEventListener("click",()=>{htmlClass.toggle("open")});const metaTheme=document.querySelector('meta[name="theme-color"]'),lightBg="#faf6f1".replace(/"/g,""),setDark=e=>{metaTheme.setAttribute("content",e?"#000":lightBg),htmlClass[e?"add":"remove"]("dark"),localStorage.setItem("dark",e),e?hljsDark.disabled=!1:hljsDark.disabled=!0},darkScheme=window.matchMedia("(prefers-color-scheme: dark)");if(htmlClass.contains("dark"))setDark(!0);else{const e=localStorage.getItem("dark");setDark(e?e==="true":darkScheme.matches)}darkScheme.addEventListener("change",e=>{setDark(e.matches)});const btnDark=document.querySelector(".btn-dark");btnDark.addEventListener("click",()=>{setDark(localStorage.getItem("dark")!=="true")})</script><div class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"><nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-6"><a class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal" href=/tags/notes/>Notes</a>
<a class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal" href=/about/>About</a></nav><nav class="mt-12 flex justify-center space-x-10 dark:invert lg:ml-12 lg:mt-0 lg:items-center lg:space-x-6"><a class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" style=--url:url(./twitter.svg) href=https://twitter.com/matei_radu target=_blank rel=me>twitter
</a><a class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" style=--url:url(./github.svg) href=https://github.com/radu-matei target=_blank rel=me>github</a></nav></div></header><main class="prose prose-neutral relative mx-auto min-h-[calc(100%-10rem)] max-w-3xl px-8 pb-24 pt-16 dark:prose-invert"><article><header class=mb-20><h1 class="!my-0 pb-2.5">TensorFlow inferencing using WebAssembly and WASI</h1><div class="text-sm opacity-60"><time>Sunday, October 18, 2020</time>
<span class=mx-1>&#183;</span>
<a class=link href=https://twitter.com/matei_radu rel=noreferrer target=_blank style=text-decoration:none><span>Radu Matei</span></a></div></header><section><p>As edge devices become more powerful, being able to perform inferencing on
trained neural network models without recompiling application components for
each architecture becomes important, and <a href=https://webassembly.org/>WebAssembly</a> could serve as the
portable compilation target for such scenarios, running both in and outside
browser environments.</p><p>In this article, we experiment with building a Rust program that performs image
classification using the <a href=https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet>MobileNet V2</a> TensorFlow model, compile it
to WebAssembly, and instantiate the module using two WebAssembly runtimes that
use the <a href=https://wasi.dev>WebAssembly System Interface (WASI)</a>, the native NodeJS WASI
runtime, and <a href=https://github.com/bytecodealliance/wasmtime>Wasmtime</a>. A special interest is given to writing model
and image data into the module&rsquo;s linear memory, with implementations in both
JavaScript and Rust. Finally, a simple prediction API is exemplified running on
top of the Wasmtime runtime, and some limitations of this approach are
discussed.</p><blockquote><p><a href=https://github.com/radu-matei/wasi-tensorflow-inference>The completed project can be found on GitHub</a>.</p></blockquote><h3 id=tensorflow-rust-and-webassembly>TensorFlow, Rust, and WebAssembly</h3><p>While there are still limitations to compiling some crates to WebAssembly, Rust
is a programming language with excellent support for Wasm. Additionally, there
is a Rust library that focuses on performing neural network model inferencing
from Rust - <a href=https://github.com/sonos/tract><code>tract</code></a>, from Sonos - and while the crate is <em>very
far from supporting any arbitrary [TensorFlow] model</em>, it can be used to run
non-trivial models, and the project has an <a href=https://github.com/sonos/tract/tree/main/examples/tensorflow-mobilenet-v2>excellent quick start
example</a> that shows how to perform image
classifications using the <a href=https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet>MobileNet V2</a> model, which will be used as
a starting point for our Wasm module:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#66d9ef>let</span> model <span style=color:#f92672>=</span> tract_tensorflow::tensorflow()
</span></span><span style=display:flex><span>    <span style=color:#75715e>// load the model
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    .model_for_path(<span style=color:#e6db74>&#34;mobilenet_v2_1.4_224_frozen.pb&#34;</span>)<span style=color:#f92672>?</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// specify input type and shape
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    .with_input_fact(
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>        InferenceFact::dt_shape(
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>f32</span>::datum_type(),
</span></span><span style=display:flex><span>            tvec!(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#75715e>// make the model runnable and fix its inputs and outputs
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    .into_runnable()<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// open image, resize it and make a Tensor out of it
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>let</span> image <span style=color:#f92672>=</span> image::open(<span style=color:#e6db74>&#34;grace_hopper.jpg&#34;</span>)<span style=color:#f92672>?</span>.to_rgb();
</span></span><span style=display:flex><span><span style=color:#66d9ef>let</span> image: <span style=color:#a6e22e>Tensor</span> <span style=color:#f92672>=</span> from_shape_fn(
</span></span><span style=display:flex><span>    (<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>3</span>),
</span></span><span style=display:flex><span>    <span style=color:#f92672>|</span>(_, y, x, c)<span style=color:#f92672>|</span> {
</span></span><span style=display:flex><span>        resized[(x <span style=color:#66d9ef>as</span> _, y <span style=color:#66d9ef>as</span> _)][c] <span style=color:#66d9ef>as</span> <span style=color:#66d9ef>f32</span> <span style=color:#f92672>/</span> <span style=color:#ae81ff>255.0</span>
</span></span><span style=display:flex><span>    })
</span></span><span style=display:flex><span>    .into();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// run the model on the input
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>let</span> result <span style=color:#f92672>=</span> model.run(tvec!(image))<span style=color:#f92672>?</span>;
</span></span></code></pre></div><blockquote><p>Source code adapted from <a href=https://github.com/sonos/tract/blob/main/examples/tensorflow-mobilenet-v2/src/main.rs>the Sonos Tract examples</a>.</p></blockquote><p>The program above loads the TensorFlow model from a file, opens and resizes the
target image to a resolution of 224 x 224 (which is the resolution of the
training images for the MobileNet model), runs the model, and prints the class
of the best prediction. Using Rust&rsquo;s <code>wasm32-wasi</code> compilation target, the
project compiles to WebAssembly successfully, but executing it with
<a href=https://github.com/bytecodealliance/wasmtime>Wasmtime</a> (or any other runtime) results in a panic:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plaintext data-lang=plaintext><span style=display:flex><span>$ wasmtime run tf-example.wasm --dir=.
</span></span><span style=display:flex><span>&#39;The global thread pool has not been initialized.:
</span></span><span style=display:flex><span>ThreadPoolBuildError { kind: IOError(Custom
</span></span><span style=display:flex><span>{ error: &#34;operation not supported on this platform&#34; })}&#39;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Caused by:
</span></span><span style=display:flex><span>    0: failed to invoke `_start`
</span></span><span style=display:flex><span>        ...
</span></span><span style=display:flex><span>         8:  rayon_core::registry::global_registry
</span></span><span style=display:flex><span>         9:  rayon_core::current_num_threads
</span></span><span style=display:flex><span>         12: jpeg_decoder::decoder::Decoder&lt;R&gt;::decode_internal
</span></span><span style=display:flex><span>         13: image::jpeg::decoder::JpegDecoder&lt;R&gt; as read_image
</span></span><span style=display:flex><span>         16: image::io::free_functions::open_impl
</span></span><span style=display:flex><span>         ...
</span></span></code></pre></div><p>The runtime error is caused by <a href=https://crates.io/crates/image>the <code>image</code> crate</a> attempting to use
multiple threads when loading the picture, and since there is no threads support
in WebAssembly (<a href=https://github.com/WebAssembly/threads>here is an early proposal</a>), the program panics.
Fortunately, turning off all crate features except JPEG loading solves the
problem:</p><pre tabindex=0><code>image = { ... default-features = false, features = [&#34;jpeg&#34;] }
</code></pre><p>Because the program assumes both the model and image are in the current
directory, we can use Wasmtime&rsquo;s <code>--dir</code> flag to grant the module permission to
the current directory, and the program classifies the image as
<code>military uniform</code> with a confidence of 32% (654 is the index of the
<code>military uniform</code> label in the labels file of the model, and the image is that
of Grace Hopper in uniform):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plaintext data-lang=plaintext><span style=display:flex><span>$ wasmtime run tf-example.wasm --dir=.
</span></span><span style=display:flex><span>result: Some((0.32560226, 654))
</span></span></code></pre></div><p>However, getting the model and image from disk on every inference is not ideal,
since I/O operations can be costly. Additionally, we might need classify images
that are not on disk, but received by the runtime in some other ways (such as
HTTP requests). In short, we need to pass both model and image data from the
runtime to the module, using WebAssembly memory.</p><h3 id=using-webassembly-memory>Using WebAssembly memory</h3><p><a href=https://webassembly.github.io/spec/core/syntax/modules.html#syntax-mem>Memory in WebAssembly</a> is represented as a contiguous vector of
uninterpreted bytes, with the memory size being a multiple of 64Ki (the length
of one <em>memory page</em>). Lin Clark has an <a href=https://hacks.mozilla.org/2017/07/memory-in-webassembly-and-why-its-safer-than-you-think/>excellent explainer with code
cartoons</a> about WebAssembly memory, and in short, we will use
it to pass arbitrary data between the Wasm runtime and guest modules.</p><p>Because the WebAssembly module is ultimately responsible for managing its own
linear memories, it must export functionality to allocate memory, which the
underlying host runtime can write into, and read from. In most cases, when using
code generation libraries such as <a href=https://github.com/rustwasm/wasm-bindgen><code>wasm-bindgen</code></a>, this is
handled by the library - but to better understand how everything works together,
it is worth building our module without <code>wasm-bindgen</code>.</p><p>Recall the task at hand - pass model and image data from the runtime into the
Wasm module&rsquo;s memory. Because both model and image data can be represented as
arrays of 8-bit unsigned integers, we can write a single function, <code>alloc</code>,
which <a href=https://doc.rust-lang.org/std/vec/struct.Vec.html#method.with_capacity>allocates memory</a> for a new <code>Vec&lt;u8></code> with capacity <code>len</code>.
Before returning, the function calls <a href=https://doc.rust-lang.org/std/mem/fn.forget.html><code>mem::forget</code></a> to take
ownership of the memory block and ensure the vector&rsquo;s destructor is not called
when the object goes out of scope. Finally, the function returns the pointer to
the start of the memory block.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#e6db74>/// Allocate memory into the module&#39;s linear memory
</span></span></span><span style=display:flex><span><span style=color:#e6db74>/// and return the offset to the start of the block.
</span></span></span><span style=display:flex><span><span style=color:#e6db74></span><span style=color:#75715e>#[no_mangle]</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>pub</span> <span style=color:#66d9ef>extern</span> <span style=color:#e6db74>&#34;C&#34;</span> <span style=color:#66d9ef>fn</span> <span style=color:#a6e22e>alloc</span>(len: <span style=color:#66d9ef>usize</span>) -&gt; <span style=color:#f92672>*</span><span style=color:#66d9ef>mut</span> <span style=color:#66d9ef>u8</span> {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>mut</span> buf <span style=color:#f92672>=</span> Vec::with_capacity(len);
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> ptr <span style=color:#f92672>=</span> buf.as_mut_ptr();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    std::mem::forget(buf);
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ptr;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>At this point, it is worth understanding how the WebAssembly module we are
writing should expect to get the pointers and length of the model and image
data - specifically, for each of the two input objects (model and image), the
module expects a pointer (offset relative to the start of its entire linear
memory) and the length of the object. Then, it uses
<a href=https://doc.rust-lang.org/std/vec/struct.Vec.html#method.from_raw_parts><code>Vec::from_raw_parts</code></a> to create a <code>Vec&lt;u8></code> with the respective
length and capacity (equal to the length) for each of the two objects:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#e6db74>/// This is the module&#39;s entry point for executing inferences.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>/// It takes as arguments pointers to the start of the module&#39;s
</span></span></span><span style=display:flex><span><span style=color:#e6db74>/// memory blocks where the model and the image were copied,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>/// as well as their lengths, meaning that callers of this
</span></span></span><span style=display:flex><span><span style=color:#e6db74>/// function must allocate memory for both the model and image
</span></span></span><span style=display:flex><span><span style=color:#e6db74>/// data using the `alloc` function, then copy it into the
</span></span></span><span style=display:flex><span><span style=color:#e6db74>/// module&#39;s linear memory at the pointers returned by `alloc`.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>///
</span></span></span><span style=display:flex><span><span style=color:#e6db74>/// It retrieves the contents of the model and image, then calls
</span></span></span><span style=display:flex><span><span style=color:#e6db74>/// the `infer` function, which performs the prediction.
</span></span></span><span style=display:flex><span><span style=color:#e6db74></span><span style=color:#75715e>#[no_mangle]</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>pub</span> <span style=color:#66d9ef>unsafe</span> <span style=color:#66d9ef>extern</span> <span style=color:#e6db74>&#34;C&#34;</span> <span style=color:#66d9ef>fn</span> <span style=color:#a6e22e>infer_from_ptrs</span>(
</span></span><span style=display:flex><span>    model_ptr: <span style=color:#f92672>*</span><span style=color:#66d9ef>mut</span> <span style=color:#66d9ef>u8</span>,
</span></span><span style=display:flex><span>    model_len: <span style=color:#66d9ef>usize</span>,
</span></span><span style=display:flex><span>    img_ptr: <span style=color:#f92672>*</span><span style=color:#66d9ef>mut</span> <span style=color:#66d9ef>u8</span>,
</span></span><span style=display:flex><span>    img_len: <span style=color:#66d9ef>usize</span>,
</span></span><span style=display:flex><span>) -&gt; <span style=color:#66d9ef>i32</span> {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> model_bytes <span style=color:#f92672>=</span> Vec::from_raw_parts(
</span></span><span style=display:flex><span>        model_ptr,
</span></span><span style=display:flex><span>        model_len,
</span></span><span style=display:flex><span>        model_len
</span></span><span style=display:flex><span>    );
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> img_bytes <span style=color:#f92672>=</span> Vec::from_raw_parts(
</span></span><span style=display:flex><span>        img_ptr,
</span></span><span style=display:flex><span>        img_len,
</span></span><span style=display:flex><span>        img_len
</span></span><span style=display:flex><span>    );
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> infer(<span style=color:#f92672>&amp;</span>model_bytes, <span style=color:#f92672>&amp;</span>img_bytes);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>As the comment suggests, host runtimes will have to call the module&rsquo;s <code>alloc</code>
function for each of the two input objects, get the respective pointers returned
by <code>alloc</code>, and copy the model and image data into the module&rsquo;s linear memory.
Then, use the pointers and lengths to call <code>infer_from_ptrs</code>, which acts as the
entrypoint for our module.</p><p>Finally, the only thing left to implement is the actual inference, which is
similar to the example we saw earlier, with the only difference that it now
takes the model and image data as arguments, rather than reading them from the
file system (hence the slight changes in loading them):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#e6db74>/// Perform the inference given the contents of the
</span></span></span><span style=display:flex><span><span style=color:#e6db74>/// model and the image, and return the index of the
</span></span></span><span style=display:flex><span><span style=color:#e6db74>/// predicted class.
</span></span></span><span style=display:flex><span><span style=color:#e6db74></span><span style=color:#66d9ef>fn</span> <span style=color:#a6e22e>infer</span>(model_bytes: <span style=color:#66d9ef>&amp;</span>[<span style=color:#66d9ef>u8</span>], image_bytes: <span style=color:#66d9ef>&amp;</span>[<span style=color:#66d9ef>u8</span>]) -&gt; <span style=color:#66d9ef>i32</span> {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>mut</span> model <span style=color:#f92672>=</span> std::io::Cursor::new(model_bytes);
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> model <span style=color:#f92672>=</span> tract_tensorflow::tensorflow()
</span></span><span style=display:flex><span>        .model_for_read(<span style=color:#f92672>&amp;</span><span style=color:#66d9ef>mut</span> model)
</span></span><span style=display:flex><span>    <span style=color:#f92672>..</span>.
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> image <span style=color:#f92672>=</span> image::load_from_memory(image_bytes);
</span></span><span style=display:flex><span>    <span style=color:#f92672>..</span>.
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> result <span style=color:#f92672>=</span> model.run(tvec!(image));
</span></span><span style=display:flex><span>    <span style=color:#f92672>..</span>.
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>We can now compile the code to the Rust <code>wasm32-wasi</code> target and get a
WebAssembly module that can be instantiated and executed in any WASI compatible
runtime, and in the following section we will explore running it in Node&rsquo;s WASI
runtime and Wasmtime.</p><blockquote><p><a href=https://github.com/radu-matei/wasi-tensorflow-inference>The complete implementation can be found on GitHub</a>.</p></blockquote><h3 id=testing-the-module-from-nodes-wasi-runtime>Testing the module from Node&rsquo;s WASI runtime</h3><p>This section will not focus on instantiating the module, but rather on
allocating and copying the model and image data into the module&rsquo;s memory, as
well as on invoking the inferencing function. For a guide on getting started
with the NodeJS WASI runtime, you can check <a href=https://radu-matei.com/blog/nodejs-wasi/>the article I wrote in
July</a>.</p><p>First, we need a helper method that copies a byte array into the module&rsquo;s
memory. The function takes as arguments the actual bytes to copy and an instance
of the module, calls the module&rsquo;s exported <code>alloc</code> function, and copies the
bytes into the module&rsquo;s memory, starting at <code>ptr</code>:</p><pre tabindex=0><code>// write `bytes` into the memory of `instance`
function writeGuestMemory(bytes, instance) {
  var len = bytes.byteLength;
  // call the module&#39;s `alloc` function
  var ptr = instance.exports.alloc(len);
  // create an array of length `len` in the module&#39;s memory,
  // starting at offset `ptr`
  var m = new Uint8Array(instance.exports.memory.buffer, ptr, len);
  // set the value of the array to `bytes`
  m.set(new Uint8Array(bytes.buffer));
  // return the offset
  return ptr;
}
</code></pre><p>Now that we have a way of writing data into the module&rsquo;s memory, we can use it
to finally get predictions on images. The function below takes as parameters the
contents of the MobilNet V2 model, the contents of an image, and a WebAssembly
instance, calls the <code>writeGuestMemory</code> function defined above for each of the
two objects we want to write into memory, then invokes the module&rsquo;s
<code>infer_from_ptrs</code> exported function using the pointers to the objects and their
lengths as parameters. The return value of the <code>infer_from_ptrs</code> function is the
index of the predicted class - we use it together with the model&rsquo;s labels file
in order to get a human-friendly description of the prediction:</p><pre tabindex=0><code>// get a prediction given the MobileNet V2 model,
// an image, and an instance of the Wasm module
// that performs the inference.
function getPrediction(model_bytes, img_bytes, instance) {
  // write the contents of the model and image
  // in the memory of the module, using the
  // module&#39;s `alloc`exported function
  var mptr = writeGuestMemory(model_bytes, instance);
  var iptr = writeGuestMemory(img_bytes, instance);

  // invoke the module&#39;s exported `infer_from_ptrs`
  // function using the pointers and lengths of
  // the model and image, which returns an integer
  //  representing the index of the predicted class.
  let pred = instance.exports.infer_from_ptrs(
    mptr,
    model_bytes.length,
    iptr,
    model_bytes.length
  );

  // helper function to read the prediction
  // label from the index of the class
  return getLabel(pred);
}
</code></pre><p>The <a href=https://github.com/radu-matei/wasi-tensorflow-inference>GitHub repository</a> of this project contains the complete logic for
instantiating the module, a compiled version of the WebAssembly module, the
MobilNet V2 model, as well as a directory with a couple of images we can test:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plaintext data-lang=plaintext><span style=display:flex><span>$ node --experimental-wasi-unstable-preview1 \
</span></span><span style=display:flex><span>       --experimental-wasm-bigint \
</span></span><span style=display:flex><span>       test.js
</span></span><span style=display:flex><span>predicting on file  golden-retriever.jpeg
</span></span><span style=display:flex><span>inference time:  832 ms
</span></span><span style=display:flex><span>prediction:  golden retriever
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>predicting on file  husky.jpeg
</span></span><span style=display:flex><span>inference time:  541 ms
</span></span><span style=display:flex><span>prediction:  Eskimo dog, husky
</span></span></code></pre></div><p>The program correctly uses the MobileNet V2 computer vision model and the
WebAssembly module we wrote, and performs image classification on the test
images.</p><h3 id=building-a-rust-host-runtime-with-wasmtime>Building a Rust host runtime with Wasmtime</h3><p>When first experimenting with compiling the module, we used the Wasmtime CLI to
quickly execute the module. This works great if the module needs to access
files, for example, but we now need to write into the module&rsquo;s memory, and we
cannot achieve that using the CLI - we now have to use the <a href=https://docs.rs/wasmtime/0.20.0/wasmtime/>Wasmtime&rsquo;s excellent
API</a>. First, we need to implement a Rust function which, given a
byte array and an instance of the WebAssembly module, writes the data into the
module&rsquo;s memory - essentially the same functionality we implemented in
JavaScript in the previous section:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#e6db74>/// Write a byte array into the instance&#39;s linear memory
</span></span></span><span style=display:flex><span><span style=color:#e6db74>/// and return the offset relative to the module&#39;s memory.
</span></span></span><span style=display:flex><span><span style=color:#e6db74></span><span style=color:#66d9ef>fn</span> <span style=color:#a6e22e>write_guest_memory</span>(
</span></span><span style=display:flex><span>    bytes: <span style=color:#66d9ef>&amp;</span>Vec<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>u8</span><span style=color:#f92672>&gt;</span>,
</span></span><span style=display:flex><span>    instance: <span style=color:#66d9ef>&amp;</span><span style=color:#a6e22e>Instance</span>
</span></span><span style=display:flex><span>) -&gt; Result<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>isize</span>, anyhow::Error<span style=color:#f92672>&gt;</span> {
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Get the &#34;memory&#34; export of the module.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#75715e>// If the module does not export it, just panic, since
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#75715e>// we are not going to be able to copy any data.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>let</span> memory <span style=color:#f92672>=</span> instance
</span></span><span style=display:flex><span>        .get_memory(<span style=color:#66d9ef>MEMORY</span>)
</span></span><span style=display:flex><span>        .expect(<span style=color:#e6db74>&#34;expected memory not found&#34;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Get the guest&#39;s exported `alloc` function, and call
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#75715e>// it with the length of the byte array.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>let</span> alloc <span style=color:#f92672>=</span> instance
</span></span><span style=display:flex><span>        .get_func(<span style=color:#66d9ef>ALLOC_FN</span>)
</span></span><span style=display:flex><span>        .expect(<span style=color:#e6db74>&#34;expected alloc function not found&#34;</span>);
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> alloc_result <span style=color:#f92672>=</span> alloc.call(
</span></span><span style=display:flex><span>        <span style=color:#f92672>&amp;</span>vec![Val::from(bytes.len() <span style=color:#66d9ef>as</span> <span style=color:#66d9ef>i32</span>)]
</span></span><span style=display:flex><span>    )<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// The result is an offset relative to the module&#39;s
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#75715e>// linear memory, which is used to copy the bytes into
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#75715e>// the module&#39;s memory.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>let</span> guest_ptr_offset <span style=color:#f92672>=</span> <span style=color:#66d9ef>match</span> alloc_result
</span></span><span style=display:flex><span>        .get(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        .expect(<span style=color:#e6db74>&#34;expected the result to have one value&#34;</span>)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        Val::I32(val) <span style=color:#f92672>=&gt;</span> <span style=color:#f92672>*</span>val <span style=color:#66d9ef>as</span> <span style=color:#66d9ef>isize</span>,
</span></span><span style=display:flex><span>        _ <span style=color:#f92672>=&gt;</span> <span style=color:#66d9ef>return</span> Err(<span style=color:#e6db74>&#34;guest pointer must be Val::I32&#34;</span>),
</span></span><span style=display:flex><span>    };
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Copy the desired bytes into the memory at `guest_ptr_offset`.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>unsafe</span> {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> raw <span style=color:#f92672>=</span> memory.data_ptr().offset(guest_ptr_offset);
</span></span><span style=display:flex><span>        raw.copy_from(bytes.as_ptr(), bytes.len());
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> Ok(guest_ptr_offset);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>We can now use the <code>write_guest_memory</code> function to implement getting a
prediction - given the contents of the model and an image, create a new
WebAssembly instance of the module using the Wasmtime API, write the contents of
the model and image into the guest&rsquo;s memory, then invoke the <code>invoke_from_ptrs</code>
function from the WebAssembly module. Finally, use a simple helper function that
gets the index of the predicted class and return the human-friendly label of the
class:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#e6db74>/// Get a prediction given the MobileNet V2 model and an image.
</span></span></span><span style=display:flex><span><span style=color:#e6db74></span><span style=color:#66d9ef>fn</span> <span style=color:#a6e22e>get_prediction</span>(
</span></span><span style=display:flex><span>    model_bytes: Vec<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>u8</span><span style=color:#f92672>&gt;</span>,
</span></span><span style=display:flex><span>    img_bytes: Vec<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>u8</span><span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>) -&gt; Result<span style=color:#f92672>&lt;</span>String, anyhow::Error<span style=color:#f92672>&gt;</span> {
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Unfortunately, we have to create a new module
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#75715e>// instance for every prediction, since a
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#75715e>// `Wasmtime::Instance` cannot be safely sent between threads.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#75715e>// See https://github.com/bytecodealliance/wasmtime/issues/793
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>let</span> instance <span style=color:#f92672>=</span> create_instance(<span style=color:#66d9ef>WASM</span>.to_string())<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Write the MobileNet model and the image contents to
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#75715e>// the module&#39;s linear memory, and get their pointers.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>let</span> model_bytes_ptr <span style=color:#f92672>=</span> write_guest_memory(
</span></span><span style=display:flex><span>        <span style=color:#f92672>&amp;</span>model_bytes,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&amp;</span>instance
</span></span><span style=display:flex><span>    )<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> img_bytes_ptr <span style=color:#f92672>=</span> write_guest_memory(
</span></span><span style=display:flex><span>        <span style=color:#f92672>&amp;</span>img_bytes,
</span></span><span style=display:flex><span>        <span style=color:#f92672>&amp;</span>instance
</span></span><span style=display:flex><span>    )<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Get the module&#39;s &#34;infer_from_ptrs&#34; function,
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#75715e>// which is the entrypoint for our module.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>let</span> infer <span style=color:#f92672>=</span> instance
</span></span><span style=display:flex><span>        .get_func(<span style=color:#66d9ef>INFER_FN</span>)
</span></span><span style=display:flex><span>        .expect(<span style=color:#e6db74>&#34;expected inference function not found&#34;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Call the inference function with the pointer
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#75715e>// and length of the model contents and image.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>let</span> results <span style=color:#f92672>=</span> infer.call(<span style=color:#f92672>&amp;</span>vec![
</span></span><span style=display:flex><span>        Val::from(model_bytes_ptr <span style=color:#66d9ef>as</span> <span style=color:#66d9ef>i32</span>),
</span></span><span style=display:flex><span>        Val::from(model_bytes.len() <span style=color:#66d9ef>as</span> <span style=color:#66d9ef>i32</span>),
</span></span><span style=display:flex><span>        Val::from(img_bytes_ptr <span style=color:#66d9ef>as</span> <span style=color:#66d9ef>i32</span>),
</span></span><span style=display:flex><span>        Val::from(img_bytes.len() <span style=color:#66d9ef>as</span> <span style=color:#66d9ef>i32</span>),
</span></span><span style=display:flex><span>    ])<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// The inference function has one return argument,
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#75715e>// the index of the predicted class.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>match</span> results
</span></span><span style=display:flex><span>        .get(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        .expect(<span style=color:#e6db74>&#34;expected the result to have one value&#34;</span>)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        Val::I32(val) <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>let</span> label <span style=color:#f92672>=</span> get_label(<span style=color:#f92672>*</span>val <span style=color:#66d9ef>as</span> <span style=color:#66d9ef>usize</span>)<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> Ok(label);
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        _ <span style=color:#f92672>=&gt;</span> <span style=color:#66d9ef>return</span> Err(<span style=color:#e6db74>&#34;cannot get prediction&#34;</span>),
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>As the comment suggests, <a href=https://github.com/bytecodealliance/wasmtime/issues/793>a <code>Wasmtime::Instance</code> cannot be sent across
threads</a> (not in a memory-safe manner) - which means we cannot
use the same instance in a multi-threaded environment (for example when
responding to HTTP requests). The impact of this is that we pay the price of
instantiating the module every time we make an inference (which, for this
specific WebAssembly module, and tested on my hardware, can be around 600-800
milliseconds).</p><p>The two implementations of executing predictions (in JavaScript and Rust) are
very similar - we are essentially performing the same operations: create
instance, write data into memory, invoke inference function.</p><blockquote><p><a href=https://github.com/radu-matei/wasi-tensorflow-inference>The complete implementation can be found on GitHub</a>.</p></blockquote><h3 id=creating-a-simple-prediction-api>Creating a simple prediction API</h3><p>Now that we are able to perform predictions using the module we built with
Wasmtime, we can easily expose this functionality over an HTTP API, using the
Rust <a href=https://docs.rs/hyper/0.13.8/hyper/><code>hyper</code></a> crate.</p><p>Running <code>cargo run --release</code> at the root of the <a href=https://github.com/radu-matei/wasi-tensorflow-inference>GitHub repository of this
project</a>, an HTTP server is started on port 3000 which expects an image
URL as request body, downloads the image contents, gets a prediction using the
method described above, then returns its human-friendly label:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plaintext data-lang=plaintext><span style=display:flex><span>$ cargo run --release
</span></span><span style=display:flex><span>Listening on http://127.0.0.1:3000
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>module instantiation time: 774.715145ms
</span></span><span style=display:flex><span>inference time: 723.531083ms
</span></span></code></pre></div><p>In another terminal instance (or from an HTTP request builder, such as Postman):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plaintext data-lang=plaintext><span style=display:flex><span>$ curl --request GET &#39;localhost:3000&#39; \
</span></span><span style=display:flex><span>--header &#39;Content-Type: text/plain&#39; \
</span></span><span style=display:flex><span>--data-raw &#39;https://&lt;url-to-a-retriever-puppy&gt;.jpg&#39;
</span></span><span style=display:flex><span>golden retriever
</span></span></code></pre></div><p>For each request, the module instantiation and inference time are printed to the
console, and this is where not being able to share a <code>Wasmtime::Instance</code> across
threads affects the overall response time of the API we built. But as
WebAssembly and WASI mature, hardware devices (such as GPUs) will be available
in WebAssembly, and as proposals such as <a href=https://www.w3.org/2020/06/machine-learning-workshop/talks/introducing_wasi_nn.html>WASI-NN</a> are implemented, we
can expect the both the instantiation and inference times to decrease.</p><p>In this article we experimented with compiling a Rust program that performs
inferencing on a pre-trained neural network to WebAssembly, and executed it in
NodeJS and Wasmtime, while closely exploring how the data is shared between the
host and guest using WebAssembly memory.</p><h3 id=faq>FAQ</h3><ul><li>Should I use this in production? Probably not, as the code presented here is
experimental, using an approach mostly suited for educational purposes.</li><li>How does this compare to <a href=https://blog.tensorflow.org/2020/03/introducing-webassembly-backend-for-tensorflow-js.html>the TensorFlow.js WebAssembly backend</a>? The
Wasm backend for TensorFlow.js can only be used from JavaScript environments,
browsers and NodeJS. This means it can&rsquo;t be used with other WebAssembly
runtimes. Check <a href=https://github.com/tensorflow/tfjs/tree/master/tfjs-backend-wasm>the official documentation for running MobileNet in the
browser, using the WebAssembly backend</a>.</li><li>How easily can I use my own model? While the <em>approach</em> used by this project
can be used to execute inferences using different neural network models, the
implementation is specialized for using the MobileNet V2 model, and changing
the model architecture or its inputs would require changes in the WebAssembly
module and its instantiation.</li></ul></section><footer class="mt-12 flex flex-wrap"><a class="mb-1.5 mr-1.5 rounded-lg bg-black/[3%] px-5 py-1.5 no-underline dark:bg-white/[8%]" href=https://radu-matei.com/tags/wasm>wasm</a>
<a class="mb-1.5 mr-1.5 rounded-lg bg-black/[3%] px-5 py-1.5 no-underline dark:bg-white/[8%]" href=https://radu-matei.com/tags/rust>rust</a>
<a class="mb-1.5 mr-1.5 rounded-lg bg-black/[3%] px-5 py-1.5 no-underline dark:bg-white/[8%]" href=https://radu-matei.com/tags/tensorflow>tensorflow</a>
<a class="mb-1.5 mr-1.5 rounded-lg bg-black/[3%] px-5 py-1.5 no-underline dark:bg-white/[8%]" href=https://radu-matei.com/tags/ml>ml</a></footer><nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]"><a class="flex w-1/2 items-center rounded-l-md p-6 pr-3 no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]" href=https://radu-matei.com/blog/a-simple-wasm-linker-js/><span class=mr-1.5>←</span><span>A simple WebAssembly linker in JavaScript</span></a>
<a class="ml-auto flex w-1/2 items-center justify-end rounded-r-md p-6 pl-3 no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]" href=https://radu-matei.com/blog/towards-sockets-networking-wasi/><span>Towards sockets and networking in WebAssembly and WASI</span><span class=ml-1.5>→</span></a></nav></article></main><footer class="opaco mx-auto flex h-[5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"><div class=mr-auto>&copy; 2024
<a class=link href=https://twitter.com/matei_radu rel=noreferrer target=_blank>Radu Matei</a></div></footer></body></html>
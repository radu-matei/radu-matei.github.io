<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"  lang="en-us" >

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" /> 
    <title>TensorFlow inferencing using WebAssembly and WASI | radu&#39;s blog</title>
     
    

    <meta name='twitter:card' content='summary'>
<meta name='twitter:site' content='@matei_radu'>
<meta name="twitter:image" content="https://radu-matei.com">
<meta name="twitter:title" content="TensorFlow inferencing using WebAssembly and WASI" />
<meta name="twitter:description" content="In this article, we experiment with building a Rust program that performs image classification using the MobileNet V2 TensorFlow model, compile it to WebAssembly, and instantiate the module using two WebAssembly runtimes that use the WebAssembly System Interface (WASI), the native NodeJS WASI runtime, and Wasmtime. A special interest is given to writing model and image data into the module’s linear memory, with implementations in both JavaScript and Rust. Finally, a simple prediction API is exemplified running on top of the Wasmtime runtime, and some limitations of this approach are discussed." />

<meta name="description" content="In this article, we experiment with building a Rust program that performs image classification using the MobileNet V2 TensorFlow model, compile it to WebAssembly, and instantiate the module using two WebAssembly runtimes that use the WebAssembly System Interface (WASI), the native NodeJS WASI runtime, and Wasmtime. A special interest is given to writing model and image data into the module’s linear memory, with implementations in both JavaScript and Rust. Finally, a simple prediction API is exemplified running on top of the Wasmtime runtime, and some limitations of this approach are discussed." />
    <meta property='og:title' content='TensorFlow inferencing using WebAssembly and WASI - radu&#39;s blog'>
<meta property='og:description' content='In this article, we experiment with building a Rust program that performs image classification using the MobileNet V2 TensorFlow model, compile it to WebAssembly, and instantiate the module using two WebAssembly runtimes that use the WebAssembly System Interface (WASI), the native NodeJS WASI runtime, and Wasmtime. A special interest is given to writing model and image data into the module’s linear memory, with implementations in both JavaScript and Rust. Finally, a simple prediction API is exemplified running on top of the Wasmtime runtime, and some limitations of this approach are discussed.'>
<meta property='og:url' content='https://radu-matei.com/blog/tensorflow-inferencing-wasi/'>
<meta property='og:site_name' content='radu&#39;s blog'>
<meta property='og:type' content='article'><meta property='og:image' content='https://www.gravatar.com/avatar/031fa2ff2832edcb6b30c8ffb61da4d4?s=256'><meta property='article:section' content='Blog'><meta property='article:tag' content='wasm'><meta property='article:tag' content='rust'><meta property='article:tag' content='tensorflow'><meta property='article:tag' content='ml'><meta property='article:published_time' content='2020-10-18T00:00:00Z'/><meta property='article:modified_time' content='2020-10-18T00:00:00Z'/>
    <link href="https://radu-matei.com/index.xml" rel="alternate" type="application/rss+xml" title="radu&#39;s blog" /> <link rel="stylesheet" href="/css/style.css">
    <link rel="stylesheet" href="/css/font-awesome.min.css">

    <link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap" rel="stylesheet">
    

    <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro&display=swap" rel="stylesheet">


    <link rel="stylesheet" href="/css/github-gist.css">
    <link disabled id="dark-mode-theme" rel="stylesheet" href="/css/dark.css">
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico">
    
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff"> 
    <link rel="canonical" href="https://radu-matei.com/blog/tensorflow-inferencing-wasi/"> 

</head>

<body>
 <section class="section" id="header">
  <div class="container">
    <nav class="nav">
      
      

      <a class="nav-item" href="https://radu-matei.com">
        <h1 class="title is-4">radu&#39;s blog</h1>
      </a>
      

      <a class="nav-item" href="https://radu-matei.com/about">
        <h1 class="title is-5">about</h1>
      </a>

      <div class="nav-right">
        <nav class="nav-item level is-mobile">
          <i class="far fa-moon" id="dark-mode-toggle" style="font-size: 21px;"></i><a class="level-item" aria-label="twitter" title=Twitter
            href="https://twitter.com/matei_radu" target="_blank"
            rel="noopener">
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" aria-label="github" title=Github
            href="https://github.com/radu-matei" target="_blank"
            rel="noopener">
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/>
    
  </svg></i>
            </span>
          </a></nav>
      </div>
    </nav>

    
    <p class="author">
      
    </p>
  </div>
</section>

<section class="section">
    <div class="container">
        <div class="subtitle tags is-6 is-pulled-right">
             
<a class="subtitle is-6" href="/tags/wasm">#wasm</a>



  
  | <a class="subtitle is-6" href="/tags/rust">#rust</a>
  
  | <a class="subtitle is-6" href="/tags/tensorflow">#tensorflow</a>
  
  | <a class="subtitle is-6" href="/tags/ml">#ml</a>
  
 
        </div>
        
        <h2 class="subtitle is-6"> October 18, 2020. 14 minutes read.   <a href="/pdf/tensorflow-inferencing-wasi.pdf" target="_blank">PDF  </a>  </h2> 

        <style type="text/css">
            .brxsmall {
                display: block;
                margin-bottom: -.9em;
            }
        </style>
        <span class="brxsmall"></span>
        <h1 class="title">TensorFlow inferencing using WebAssembly and WASI</h1>

        
        <div class="content">
            


            <p>As edge devices become more powerful, being able to perform inferencing on
trained neural network models without recompiling application components for
each architecture becomes important, and <a href="https://webassembly.org/" target="_blank" rel="noreferrer noopener">WebAssembly</a>
 could serve as the
portable compilation target for such scenarios, running both in and outside
browser environments.</p>
<p>In this article, we experiment with building a Rust program that performs image
classification using the <a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet" target="_blank" rel="noreferrer noopener">MobileNet V2</a>
 TensorFlow model, compile it
to WebAssembly, and instantiate the module using two WebAssembly runtimes that
use the <a href="https://wasi.dev" target="_blank" rel="noreferrer noopener">WebAssembly System Interface (WASI)</a>
, the native NodeJS WASI
runtime, and <a href="https://github.com/bytecodealliance/wasmtime" target="_blank" rel="noreferrer noopener">Wasmtime</a>
. A special interest is given to writing model
and image data into the module&rsquo;s linear memory, with implementations in both
JavaScript and Rust. Finally, a simple prediction API is exemplified running on
top of the Wasmtime runtime, and some limitations of this approach are
discussed.</p>
<blockquote>
<p><a href="https://github.com/radu-matei/wasi-tensorflow-inference" target="_blank" rel="noreferrer noopener">The completed project can be found on GitHub</a>
.</p>
</blockquote>
<h3 id="tensorflow-rust-and-webassembly">TensorFlow, Rust, and WebAssembly</h3>
<p>While there are still limitations to compiling some crates to WebAssembly, Rust
is a programming language with excellent support for Wasm. Additionally, there
is a Rust library that focuses on performing neural network model inferencing
from Rust - <a href="https://github.com/sonos/tract" target="_blank" rel="noreferrer noopener"><code>tract</code></a>
, from Sonos - and while the crate is <em>very
far from supporting any arbitrary [TensorFlow] model</em>, it can be used to run
non-trivial models, and the project has an <a href="https://github.com/sonos/tract/tree/main/examples/tensorflow-mobilenet-v2" target="_blank" rel="noreferrer noopener">excellent quick start
example</a>
 that shows how to perform image
classifications using the <a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet" target="_blank" rel="noreferrer noopener">MobileNet V2</a>
 model, which will be used as
a starting point for our Wasm module:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#66d9ef">let</span> model <span style="color:#f92672">=</span> tract_tensorflow::tensorflow()
    <span style="color:#75715e">// load the model
</span><span style="color:#75715e"></span>    .model_for_path(<span style="color:#e6db74">&#34;mobilenet_v2_1.4_224_frozen.pb&#34;</span>)<span style="color:#f92672">?</span>
    <span style="color:#75715e">// specify input type and shape
</span><span style="color:#75715e"></span>    .with_input_fact(
        <span style="color:#ae81ff">0</span>,
        InferenceFact::dt_shape(
            <span style="color:#66d9ef">f32</span>::datum_type(),
            tvec<span style="color:#f92672">!</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">3</span>)
        ),
    )
    <span style="color:#75715e">// make the model runnable and fix its inputs and outputs
</span><span style="color:#75715e"></span>    .into_runnable()<span style="color:#f92672">?</span>;

<span style="color:#75715e">// open image, resize it and make a Tensor out of it
</span><span style="color:#75715e"></span><span style="color:#66d9ef">let</span> image <span style="color:#f92672">=</span> image::open(<span style="color:#e6db74">&#34;grace_hopper.jpg&#34;</span>)<span style="color:#f92672">?</span>.to_rgb();
<span style="color:#66d9ef">let</span> image: <span style="color:#a6e22e">Tensor</span> <span style="color:#f92672">=</span> from_shape_fn(
    (<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">3</span>),
    <span style="color:#f92672">|</span>(_, y, x, c)<span style="color:#f92672">|</span> {
        resized[(x <span style="color:#66d9ef">as</span> _, y <span style="color:#66d9ef">as</span> _)][c] <span style="color:#66d9ef">as</span> <span style="color:#66d9ef">f32</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>
    })
    .into();

<span style="color:#75715e">// run the model on the input
</span><span style="color:#75715e"></span><span style="color:#66d9ef">let</span> result <span style="color:#f92672">=</span> model.run(tvec<span style="color:#f92672">!</span>(image))<span style="color:#f92672">?</span>;
</code></pre></div><blockquote>
<p>Source code adapted from <a href="https://github.com/sonos/tract/blob/main/examples/tensorflow-mobilenet-v2/src/main.rs" target="_blank" rel="noreferrer noopener">the Sonos Tract examples</a>
.</p>
</blockquote>
<p>The program above loads the TensorFlow model from a file, opens and resizes the
target image to a resolution of 224 x 224 (which is the resolution of the
training images for the MobileNet model), runs the model, and prints the class
of the best prediction. Using Rust&rsquo;s <code>wasm32-wasi</code> compilation target, the
project compiles to WebAssembly successfully, but executing it with
<a href="https://github.com/bytecodealliance/wasmtime" target="_blank" rel="noreferrer noopener">Wasmtime</a>
 (or any other runtime) results in a panic:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-plaintext" data-lang="plaintext">$ wasmtime run tf-example.wasm --dir=.
&#39;The global thread pool has not been initialized.:
ThreadPoolBuildError { kind: IOError(Custom
{ error: &#34;operation not supported on this platform&#34; })}&#39;

Caused by:
    0: failed to invoke `_start`
        ...
         8:  rayon_core::registry::global_registry
         9:  rayon_core::current_num_threads
         12: jpeg_decoder::decoder::Decoder&lt;R&gt;::decode_internal
         13: image::jpeg::decoder::JpegDecoder&lt;R&gt; as read_image
         16: image::io::free_functions::open_impl
         ...
</code></pre></div><p>The runtime error is caused by <a href="https://crates.io/crates/image" target="_blank" rel="noreferrer noopener">the <code>image</code> crate</a>
 attempting to use
multiple threads when loading the picture, and since there is no threads support
in WebAssembly (<a href="https://github.com/WebAssembly/threads" target="_blank" rel="noreferrer noopener">here is an early proposal</a>
), the program panics.
Fortunately, turning off all crate features except JPEG loading solves the
problem:</p>
<pre><code>image = { ... default-features = false, features = [&quot;jpeg&quot;] }
</code></pre><p>Because the program assumes both the model and image are in the current
directory, we can use Wasmtime&rsquo;s <code>--dir</code> flag to grant the module permission to
the current directory, and the program classifies the image as
<code>military uniform</code> with a confidence of 32% (654 is the index of the
<code>military uniform</code> label in the labels file of the model, and the image is that
of Grace Hopper in uniform):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-plaintext" data-lang="plaintext">$ wasmtime run tf-example.wasm --dir=.
result: Some((0.32560226, 654))
</code></pre></div><p>However, getting the model and image from disk on every inference is not ideal,
since I/O operations can be costly. Additionally, we might need classify images
that are not on disk, but received by the runtime in some other ways (such as
HTTP requests). In short, we need to pass both model and image data from the
runtime to the module, using WebAssembly memory.</p>
<h3 id="using-webassembly-memory">Using WebAssembly memory</h3>
<p><a href="https://webassembly.github.io/spec/core/syntax/modules.html#syntax-mem" target="_blank" rel="noreferrer noopener">Memory in WebAssembly</a>
 is represented as a contiguous vector of
uninterpreted bytes, with the memory size being a multiple of 64Ki (the length
of one <em>memory page</em>). Lin Clark has an <a href="https://hacks.mozilla.org/2017/07/memory-in-webassembly-and-why-its-safer-than-you-think/" target="_blank" rel="noreferrer noopener">excellent explainer with code
cartoons</a>
 about WebAssembly memory, and in short, we will use
it to pass arbitrary data between the Wasm runtime and guest modules.</p>
<p>Because the WebAssembly module is ultimately responsible for managing its own
linear memories, it must export functionality to allocate memory, which the
underlying host runtime can write into, and read from. In most cases, when using
code generation libraries such as <a href="https://github.com/rustwasm/wasm-bindgen" target="_blank" rel="noreferrer noopener"><code>wasm-bindgen</code></a>
, this is
handled by the library - but to better understand how everything works together,
it is worth building our module without <code>wasm-bindgen</code>.</p>
<p>Recall the task at hand - pass model and image data from the runtime into the
Wasm module&rsquo;s memory. Because both model and image data can be represented as
arrays of 8-bit unsigned integers, we can write a single function, <code>alloc</code>,
which <a href="https://doc.rust-lang.org/std/vec/struct.Vec.html#method.with_capacity" target="_blank" rel="noreferrer noopener">allocates memory</a>
 for a new <code>Vec&lt;u8&gt;</code> with capacity <code>len</code>.
Before returning, the function calls <a href="https://doc.rust-lang.org/std/mem/fn.forget.html" target="_blank" rel="noreferrer noopener"><code>mem::forget</code></a>
 to take
ownership of the memory block and ensure the vector&rsquo;s destructor is not called
when the object goes out of scope. Finally, the function returns the pointer to
the start of the memory block.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#e6db74">/// Allocate memory into the module&#39;s linear memory
</span><span style="color:#e6db74">/// and return the offset to the start of the block.
</span><span style="color:#e6db74"></span><span style="color:#75715e">#[no_mangle]</span>
<span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">extern</span> <span style="color:#e6db74">&#34;C&#34;</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">alloc</span>(len: <span style="color:#66d9ef">usize</span>) -&gt; <span style="color:#f92672">*</span><span style="color:#66d9ef">mut</span> <span style="color:#66d9ef">u8</span> {
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> buf <span style="color:#f92672">=</span> Vec::with_capacity(len);
    <span style="color:#66d9ef">let</span> ptr <span style="color:#f92672">=</span> buf.as_mut_ptr();

    std::mem::forget(buf);
    <span style="color:#66d9ef">return</span> ptr;
}
</code></pre></div><p>At this point, it is worth understanding how the WebAssembly module we are
writing should expect to get the pointers and length of the model and image
data - specifically, for each of the two input objects (model and image), the
module expects a pointer (offset relative to the start of its entire linear
memory) and the length of the object. Then, it uses
<a href="https://doc.rust-lang.org/std/vec/struct.Vec.html#method.from_raw_parts" target="_blank" rel="noreferrer noopener"><code>Vec::from_raw_parts</code></a>
 to create a <code>Vec&lt;u8&gt;</code> with the respective
length and capacity (equal to the length) for each of the two objects:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#e6db74">/// This is the module&#39;s entry point for executing inferences.
</span><span style="color:#e6db74">/// It takes as arguments pointers to the start of the module&#39;s
</span><span style="color:#e6db74">/// memory blocks where the model and the image were copied,
</span><span style="color:#e6db74">/// as well as their lengths, meaning that callers of this
</span><span style="color:#e6db74">/// function must allocate memory for both the model and image
</span><span style="color:#e6db74">/// data using the `alloc` function, then copy it into the
</span><span style="color:#e6db74">/// module&#39;s linear memory at the pointers returned by `alloc`.
</span><span style="color:#e6db74">///
</span><span style="color:#e6db74">/// It retrieves the contents of the model and image, then calls
</span><span style="color:#e6db74">/// the `infer` function, which performs the prediction.
</span><span style="color:#e6db74"></span><span style="color:#75715e">#[no_mangle]</span>
<span style="color:#66d9ef">pub</span> <span style="color:#66d9ef">unsafe</span> <span style="color:#66d9ef">extern</span> <span style="color:#e6db74">&#34;C&#34;</span> <span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">infer_from_ptrs</span>(
    model_ptr: <span style="color:#f92672">*</span><span style="color:#66d9ef">mut</span> <span style="color:#66d9ef">u8</span>,
    model_len: <span style="color:#66d9ef">usize</span>,
    img_ptr: <span style="color:#f92672">*</span><span style="color:#66d9ef">mut</span> <span style="color:#66d9ef">u8</span>,
    img_len: <span style="color:#66d9ef">usize</span>,
) -&gt; <span style="color:#66d9ef">i32</span> {
    <span style="color:#66d9ef">let</span> model_bytes <span style="color:#f92672">=</span> Vec::from_raw_parts(
        model_ptr,
        model_len,
        model_len
    );
    <span style="color:#66d9ef">let</span> img_bytes <span style="color:#f92672">=</span> Vec::from_raw_parts(
        img_ptr,
        img_len,
        img_len
    );

    <span style="color:#66d9ef">return</span> infer(<span style="color:#f92672">&amp;</span>model_bytes, <span style="color:#f92672">&amp;</span>img_bytes);
}
</code></pre></div><p>As the comment suggests, host runtimes will have to call the module&rsquo;s <code>alloc</code>
function for each of the two input objects, get the respective pointers returned
by <code>alloc</code>, and copy the model and image data into the module&rsquo;s linear memory.
Then, use the pointers and lengths to call <code>infer_from_ptrs</code>, which acts as the
entrypoint for our module.</p>
<p>Finally, the only thing left to implement is the actual inference, which is
similar to the example we saw earlier, with the only difference that it now
takes the model and image data as arguments, rather than reading them from the
file system (hence the slight changes in loading them):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#e6db74">/// Perform the inference given the contents of the
</span><span style="color:#e6db74">/// model and the image, and return the index of the
</span><span style="color:#e6db74">/// predicted class.
</span><span style="color:#e6db74"></span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">infer</span>(model_bytes: <span style="color:#66d9ef">&amp;</span>[<span style="color:#66d9ef">u8</span>], image_bytes: <span style="color:#66d9ef">&amp;</span>[<span style="color:#66d9ef">u8</span>]) -&gt; <span style="color:#66d9ef">i32</span> {
    <span style="color:#66d9ef">let</span> <span style="color:#66d9ef">mut</span> model <span style="color:#f92672">=</span> std::io::Cursor::new(model_bytes);
    <span style="color:#66d9ef">let</span> model <span style="color:#f92672">=</span> tract_tensorflow::tensorflow()
        .model_for_read(<span style="color:#f92672">&amp;</span><span style="color:#66d9ef">mut</span> model)
    ...
    <span style="color:#66d9ef">let</span> image <span style="color:#f92672">=</span> image::load_from_memory(image_bytes);
    ...
    <span style="color:#66d9ef">let</span> result <span style="color:#f92672">=</span> model.run(tvec<span style="color:#f92672">!</span>(image));
    ...
}
</code></pre></div><p>We can now compile the code to the Rust <code>wasm32-wasi</code> target and get a
WebAssembly module that can be instantiated and executed in any WASI compatible
runtime, and in the following section we will explore running it in Node&rsquo;s WASI
runtime and Wasmtime.</p>
<blockquote>
<p><a href="https://github.com/radu-matei/wasi-tensorflow-inference" target="_blank" rel="noreferrer noopener">The complete implementation can be found on GitHub</a>
.</p>
</blockquote>
<h3 id="testing-the-module-from-nodes-wasi-runtime">Testing the module from Node&rsquo;s WASI runtime</h3>
<p>This section will not focus on instantiating the module, but rather on
allocating and copying the model and image data into the module&rsquo;s memory, as
well as on invoking the inferencing function. For a guide on getting started
with the NodeJS WASI runtime, you can check <a href="https://radu-matei.com/blog/nodejs-wasi/" target="_blank" rel="noreferrer noopener">the article I wrote in
July</a>
.</p>
<p>First, we need a helper method that copies a byte array into the module&rsquo;s
memory. The function takes as arguments the actual bytes to copy and an instance
of the module, calls the module&rsquo;s exported <code>alloc</code> function, and copies the
bytes into the module&rsquo;s memory, starting at <code>ptr</code>:</p>
<pre><code>// write `bytes` into the memory of `instance`
function writeGuestMemory(bytes, instance) {
  var len = bytes.byteLength;
  // call the module's `alloc` function
  var ptr = instance.exports.alloc(len);
  // create an array of length `len` in the module's memory,
  // starting at offset `ptr`
  var m = new Uint8Array(instance.exports.memory.buffer, ptr, len);
  // set the value of the array to `bytes`
  m.set(new Uint8Array(bytes.buffer));
  // return the offset
  return ptr;
}
</code></pre><p>Now that we have a way of writing data into the module&rsquo;s memory, we can use it
to finally get predictions on images. The function below takes as parameters the
contents of the MobilNet V2 model, the contents of an image, and a WebAssembly
instance, calls the <code>writeGuestMemory</code> function defined above for each of the
two objects we want to write into memory, then invokes the module&rsquo;s
<code>infer_from_ptrs</code> exported function using the pointers to the objects and their
lengths as parameters. The return value of the <code>infer_from_ptrs</code> function is the
index of the predicted class - we use it together with the model&rsquo;s labels file
in order to get a human-friendly description of the prediction:</p>
<pre><code>// get a prediction given the MobileNet V2 model,
// an image, and an instance of the Wasm module
// that performs the inference.
function getPrediction(model_bytes, img_bytes, instance) {
  // write the contents of the model and image
  // in the memory of the module, using the
  // module's `alloc`exported function
  var mptr = writeGuestMemory(model_bytes, instance);
  var iptr = writeGuestMemory(img_bytes, instance);

  // invoke the module's exported `infer_from_ptrs`
  // function using the pointers and lengths of
  // the model and image, which returns an integer
  //  representing the index of the predicted class.
  let pred = instance.exports.infer_from_ptrs(
    mptr,
    model_bytes.length,
    iptr,
    model_bytes.length
  );

  // helper function to read the prediction
  // label from the index of the class
  return getLabel(pred);
}
</code></pre><p>The <a href="https://github.com/radu-matei/wasi-tensorflow-inference" target="_blank" rel="noreferrer noopener">GitHub repository</a>
 of this project contains the complete logic for
instantiating the module, a compiled version of the WebAssembly module, the
MobilNet V2 model, as well as a directory with a couple of images we can test:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-plaintext" data-lang="plaintext">$ node --experimental-wasi-unstable-preview1 \
       --experimental-wasm-bigint \
       test.js
predicting on file  golden-retriever.jpeg
inference time:  832 ms
prediction:  golden retriever

predicting on file  husky.jpeg
inference time:  541 ms
prediction:  Eskimo dog, husky
</code></pre></div><p>The program correctly uses the MobileNet V2 computer vision model and the
WebAssembly module we wrote, and performs image classification on the test
images.</p>
<h3 id="building-a-rust-host-runtime-with-wasmtime">Building a Rust host runtime with Wasmtime</h3>
<p>When first experimenting with compiling the module, we used the Wasmtime CLI to
quickly execute the module. This works great if the module needs to access
files, for example, but we now need to write into the module&rsquo;s memory, and we
cannot achieve that using the CLI - we now have to use the <a href="https://docs.rs/wasmtime/0.20.0/wasmtime/" target="_blank" rel="noreferrer noopener">Wasmtime&rsquo;s excellent
API</a>
. First, we need to implement a Rust function which, given a
byte array and an instance of the WebAssembly module, writes the data into the
module&rsquo;s memory - essentially the same functionality we implemented in
JavaScript in the previous section:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#e6db74">/// Write a byte array into the instance&#39;s linear memory
</span><span style="color:#e6db74">/// and return the offset relative to the module&#39;s memory.
</span><span style="color:#e6db74"></span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">write_guest_memory</span>(
    bytes: <span style="color:#66d9ef">&amp;</span>Vec<span style="color:#f92672">&lt;</span><span style="color:#66d9ef">u8</span><span style="color:#f92672">&gt;</span>,
    instance: <span style="color:#66d9ef">&amp;</span><span style="color:#a6e22e">Instance</span>
) -&gt; Result<span style="color:#f92672">&lt;</span><span style="color:#66d9ef">isize</span>, anyhow::Error<span style="color:#f92672">&gt;</span> {
    <span style="color:#75715e">// Get the &#34;memory&#34; export of the module.
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// If the module does not export it, just panic, since
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// we are not going to be able to copy any data.
</span><span style="color:#75715e"></span>    <span style="color:#66d9ef">let</span> memory <span style="color:#f92672">=</span> instance
        .get_memory(MEMORY)
        .expect(<span style="color:#e6db74">&#34;expected memory not found&#34;</span>);

    <span style="color:#75715e">// Get the guest&#39;s exported `alloc` function, and call
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// it with the length of the byte array.
</span><span style="color:#75715e"></span>    <span style="color:#66d9ef">let</span> alloc <span style="color:#f92672">=</span> instance
        .get_func(ALLOC_FN)
        .expect(<span style="color:#e6db74">&#34;expected alloc function not found&#34;</span>);
    <span style="color:#66d9ef">let</span> alloc_result <span style="color:#f92672">=</span> alloc.call(
        <span style="color:#f92672">&amp;</span>vec<span style="color:#f92672">!</span>[Val::from(bytes.len() <span style="color:#66d9ef">as</span> <span style="color:#66d9ef">i32</span>)]
    )<span style="color:#f92672">?</span>;

    <span style="color:#75715e">// The result is an offset relative to the module&#39;s
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// linear memory, which is used to copy the bytes into
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// the module&#39;s memory.
</span><span style="color:#75715e"></span>    <span style="color:#66d9ef">let</span> guest_ptr_offset <span style="color:#f92672">=</span> <span style="color:#66d9ef">match</span> alloc_result
        .get(<span style="color:#ae81ff">0</span>)
        .expect(<span style="color:#e6db74">&#34;expected the result to have one value&#34;</span>)
    {
        Val::I32(val) <span style="color:#f92672">=&gt;</span> <span style="color:#f92672">*</span>val <span style="color:#66d9ef">as</span> <span style="color:#66d9ef">isize</span>,
        _ <span style="color:#f92672">=&gt;</span> <span style="color:#66d9ef">return</span> Err(<span style="color:#e6db74">&#34;guest pointer must be Val::I32&#34;</span>),
    };

    <span style="color:#75715e">// Copy the desired bytes into the memory at `guest_ptr_offset`.
</span><span style="color:#75715e"></span>    <span style="color:#66d9ef">unsafe</span> {
        <span style="color:#66d9ef">let</span> raw <span style="color:#f92672">=</span> memory.data_ptr().offset(guest_ptr_offset);
        raw.copy_from(bytes.as_ptr(), bytes.len());
    }
    <span style="color:#66d9ef">return</span> Ok(guest_ptr_offset);
}
</code></pre></div><p>We can now use the <code>write_guest_memory</code> function to implement getting a
prediction - given the contents of the model and an image, create a new
WebAssembly instance of the module using the Wasmtime API, write the contents of
the model and image into the guest&rsquo;s memory, then invoke the <code>invoke_from_ptrs</code>
function from the WebAssembly module. Finally, use a simple helper function that
gets the index of the predicted class and return the human-friendly label of the
class:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-rust" data-lang="rust"><span style="color:#e6db74">/// Get a prediction given the MobileNet V2 model and an image.
</span><span style="color:#e6db74"></span><span style="color:#66d9ef">fn</span> <span style="color:#a6e22e">get_prediction</span>(
    model_bytes: Vec<span style="color:#f92672">&lt;</span><span style="color:#66d9ef">u8</span><span style="color:#f92672">&gt;</span>,
    img_bytes: Vec<span style="color:#f92672">&lt;</span><span style="color:#66d9ef">u8</span><span style="color:#f92672">&gt;</span>
) -&gt; Result<span style="color:#f92672">&lt;</span>String, anyhow::Error<span style="color:#f92672">&gt;</span> {
    <span style="color:#75715e">// Unfortunately, we have to create a new module
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// instance for every prediction, since a
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// `Wasmtime::Instance` cannot be safely sent between threads.
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// See https://github.com/bytecodealliance/wasmtime/issues/793
</span><span style="color:#75715e"></span>    <span style="color:#66d9ef">let</span> instance <span style="color:#f92672">=</span> create_instance(WASM.to_string())<span style="color:#f92672">?</span>;

    <span style="color:#75715e">// Write the MobileNet model and the image contents to
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// the module&#39;s linear memory, and get their pointers.
</span><span style="color:#75715e"></span>    <span style="color:#66d9ef">let</span> model_bytes_ptr <span style="color:#f92672">=</span> write_guest_memory(
        <span style="color:#f92672">&amp;</span>model_bytes,
        <span style="color:#f92672">&amp;</span>instance
    )<span style="color:#f92672">?</span>;
    <span style="color:#66d9ef">let</span> img_bytes_ptr <span style="color:#f92672">=</span> write_guest_memory(
        <span style="color:#f92672">&amp;</span>img_bytes,
        <span style="color:#f92672">&amp;</span>instance
    )<span style="color:#f92672">?</span>;

    <span style="color:#75715e">// Get the module&#39;s &#34;infer_from_ptrs&#34; function,
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// which is the entrypoint for our module.
</span><span style="color:#75715e"></span>    <span style="color:#66d9ef">let</span> infer <span style="color:#f92672">=</span> instance
        .get_func(INFER_FN)
        .expect(<span style="color:#e6db74">&#34;expected inference function not found&#34;</span>);

    <span style="color:#75715e">// Call the inference function with the pointer
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// and length of the model contents and image.
</span><span style="color:#75715e"></span>    <span style="color:#66d9ef">let</span> results <span style="color:#f92672">=</span> infer.call(<span style="color:#f92672">&amp;</span>vec<span style="color:#f92672">!</span>[
        Val::from(model_bytes_ptr <span style="color:#66d9ef">as</span> <span style="color:#66d9ef">i32</span>),
        Val::from(model_bytes.len() <span style="color:#66d9ef">as</span> <span style="color:#66d9ef">i32</span>),
        Val::from(img_bytes_ptr <span style="color:#66d9ef">as</span> <span style="color:#66d9ef">i32</span>),
        Val::from(img_bytes.len() <span style="color:#66d9ef">as</span> <span style="color:#66d9ef">i32</span>),
    ])<span style="color:#f92672">?</span>;

    <span style="color:#75715e">// The inference function has one return argument,
</span><span style="color:#75715e"></span>    <span style="color:#75715e">// the index of the predicted class.
</span><span style="color:#75715e"></span>    <span style="color:#66d9ef">match</span> results
        .get(<span style="color:#ae81ff">0</span>)
        .expect(<span style="color:#e6db74">&#34;expected the result to have one value&#34;</span>)
    {
        Val::I32(val) <span style="color:#f92672">=&gt;</span> {
            <span style="color:#66d9ef">let</span> label <span style="color:#f92672">=</span> get_label(<span style="color:#f92672">*</span>val <span style="color:#66d9ef">as</span> <span style="color:#66d9ef">usize</span>)<span style="color:#f92672">?</span>;
            <span style="color:#66d9ef">return</span> Ok(label);
        }
        _ <span style="color:#f92672">=&gt;</span> <span style="color:#66d9ef">return</span> Err(<span style="color:#e6db74">&#34;cannot get prediction&#34;</span>),
    }
}
</code></pre></div><p>As the comment suggests, <a href="https://github.com/bytecodealliance/wasmtime/issues/793" target="_blank" rel="noreferrer noopener">a <code>Wasmtime::Instance</code> cannot be sent across
threads</a>
 (not in a memory-safe manner) - which means we cannot
use the same instance in a multi-threaded environment (for example when
responding to HTTP requests). The impact of this is that we pay the price of
instantiating the module every time we make an inference (which, for this
specific WebAssembly module, and tested on my hardware, can be around 600-800
milliseconds).</p>
<p>The two implementations of executing predictions (in JavaScript and Rust) are
very similar - we are essentially performing the same operations: create
instance, write data into memory, invoke inference function.</p>
<blockquote>
<p><a href="https://github.com/radu-matei/wasi-tensorflow-inference" target="_blank" rel="noreferrer noopener">The complete implementation can be found on GitHub</a>
.</p>
</blockquote>
<h3 id="creating-a-simple-prediction-api">Creating a simple prediction API</h3>
<p>Now that we are able to perform predictions using the module we built with
Wasmtime, we can easily expose this functionality over an HTTP API, using the
Rust <a href="https://docs.rs/hyper/0.13.8/hyper/" target="_blank" rel="noreferrer noopener"><code>hyper</code></a>
 crate.</p>
<p>Running <code>cargo run --release</code> at the root of the <a href="https://github.com/radu-matei/wasi-tensorflow-inference" target="_blank" rel="noreferrer noopener">GitHub repository of this
project</a>
, an HTTP server is started on port 3000 which expects an image
URL as request body, downloads the image contents, gets a prediction using the
method described above, then returns its human-friendly label:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-plaintext" data-lang="plaintext">$ cargo run --release
Listening on http://127.0.0.1:3000

module instantiation time: 774.715145ms
inference time: 723.531083ms
</code></pre></div><p>In another terminal instance (or from an HTTP request builder, such as Postman):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-plaintext" data-lang="plaintext">$ curl --request GET &#39;localhost:3000&#39; \
--header &#39;Content-Type: text/plain&#39; \
--data-raw &#39;https://&lt;url-to-a-retriever-puppy&gt;.jpg&#39;
golden retriever
</code></pre></div><p>For each request, the module instantiation and inference time are printed to the
console, and this is where not being able to share a <code>Wasmtime::Instance</code> across
threads affects the overall response time of the API we built. But as
WebAssembly and WASI mature, hardware devices (such as GPUs) will be available
in WebAssembly, and as proposals such as <a href="https://www.w3.org/2020/06/machine-learning-workshop/talks/introducing_wasi_nn.html" target="_blank" rel="noreferrer noopener">WASI-NN</a>
 are implemented, we
can expect the both the instantiation and inference times to decrease.</p>
<p>In this article we experimented with compiling a Rust program that performs
inferencing on a pre-trained neural network to WebAssembly, and executed it in
NodeJS and Wasmtime, while closely exploring how the data is shared between the
host and guest using WebAssembly memory.</p>
<h3 id="faq">FAQ</h3>
<ul>
<li>Should I use this in production? Probably not, as the code presented here is
experimental, using an approach mostly suited for educational purposes.</li>
<li>How does this compare to <a href="https://blog.tensorflow.org/2020/03/introducing-webassembly-backend-for-tensorflow-js.html" target="_blank" rel="noreferrer noopener">the TensorFlow.js WebAssembly backend</a>
? The
Wasm backend for TensorFlow.js can only be used from JavaScript environments,
browsers and NodeJS. This means it can&rsquo;t be used with other WebAssembly
runtimes. Check <a href="https://github.com/tensorflow/tfjs/tree/master/tfjs-backend-wasm" target="_blank" rel="noreferrer noopener">the official documentation for running MobileNet in the
browser, using the WebAssembly backend</a>
.</li>
<li>How easily can I use my own model? While the <em>approach</em> used by this project
can be used to execute inferences using different neural network models, the
implementation is specialized for using the MobileNet V2 model, and changing
the model architecture or its inputs would require changes in the WebAssembly
module and its instantiation.</li>
</ul>


            
            <p class="author"><a href="https://twitter.com/matei_radu"
                    target="_blank" rel="noreferrer">Radu Matei<br>
                    <small>@matei_radu</a></small>
            </p>

            
        </div>
        
    </div>
</section>


<section class="section">
  <div class="container">
    <aside>
      <div id="disqus_thread"></div>
    </aside>
    <div id="show_comments"><a id="load_comments" class="button is-link">Load comments</a></div>
    <script type="text/javascript">
      var disqus_shortname = 'radu-matei';
      function disqus() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }
      
      var hash = window.location.hash.substr(1);
      if ((hash.length > 8) && (hash.substring(0, 8) === "comment-")) {
        disqus();
        document.getElementById("show_comments").style.display = "none";
      } else {
        document.getElementById('load_comments').onclick = function () {
          disqus();
          document.getElementById("show_comments").style.display = "none";
        };
      }

    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments
        powered by Disqus.</a></noscript>
  </div>
</section>


<section class="section">
  <div class="container has-text-centered">
    <p>&copy; <a href="https://twitter.com/matei_radu" rel="noreferrer" target="_blank">Radu Matei</a> 2021</p>
    
  </div>

</section>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-81142224-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>





<script src="/js/dark.js "></script>


</body>

</html>
